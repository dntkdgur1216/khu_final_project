import os
import math
import torch
import clip
import data_utils

from tqdm import tqdm
from torch.utils.data import DataLoader

###########10/14 ì¶”ê°€ ë‚´ìš©. 10/14ëŠ” lf-cbmì— remoteclipì ìš©ì„ ìœ„í•´
import open_clip 

def load_remote_clip(model_name, checkpoint_path, device="cuda"):
    """
    open_clipìœ¼ë¡œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë§Œë“¤ê³ , ë¡œì»¬ì˜ RemoteCLIP ê°€ì¤‘ì¹˜ë¥¼ ì”Œì›ë‹ˆë‹¤.
    retrieval.pyì˜ get_model í•¨ìˆ˜ ë¡œì§ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    model, _, preprocess = open_clip.create_model_and_transforms(
        model_name=model_name,
        pretrained='openai', # êµ¬ì¡°ë§Œ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ openai pretrainì„ ì‚¬ìš©
        device=device,
        cache_dir='cache/weights/open_clip'
    )
    
    print(f"Loading RemoteCLIP weights from: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # ê°€ì¤‘ì¹˜ë¥¼ ë®ì–´ì”Œì›ë‹ˆë‹¤.
    msg = model.load_state_dict(checkpoint)
    print("RemoteCLIP weights loaded successfully.")
    print(msg) # <All keys matched successfully> ë©”ì‹œì§€ê°€ ëœ¨ëŠ”ì§€ í™•ì¸
    
    return model, preprocess
###########10/14 ì¶”ê°€ ë‚´ìš©. 10/14ëŠ” lf-cbmì— remoteclipì ìš©ì„ ìœ„í•´

PM_SUFFIX = {"max":"_max", "avg":""}
#############10/13 ì¶”ê°€ë‚´ìš©
'''
def save_target_activations(target_model, dataset, save_name, target_layers = ["layer4"], batch_size = 1000,
                            device = "cuda", pool_mode='avg'):
    """
    save_name: save_file path, should include {} which will be formatted by layer names
    """
    _make_save_dir(save_name)
    save_names = {}    
    for target_layer in target_layers:
        save_names[target_layer] = save_name.format(target_layer)
        
    if _all_saved(save_names):
        return
    
    all_features = {target_layer:[] for target_layer in target_layers}
    
    hooks = {}
    for target_layer in target_layers:
        command = "target_model.{}.register_forward_hook(get_activation(all_features[target_layer], pool_mode))".format(target_layer)
        hooks[target_layer] = eval(command)
    
    with torch.no_grad():
        for images, labels in tqdm(DataLoader(dataset, batch_size, num_workers=8, pin_memory=True)):
            features = target_model(images.to(device))
    
    for target_layer in target_layers:
        torch.save(torch.cat(all_features[target_layer]), save_names[target_layer])
        hooks[target_layer].remove()
    #free memory
    del all_features
    torch.cuda.empty_cache()
    return
'''
# utils.py íŒŒì¼ì˜ save_target_activations í•¨ìˆ˜ë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•˜ì„¸ìš”.

def save_target_activations(target_model, data, save_names, target_layers, device,
                            target_name, batch_size=128):
    
    # ğŸŒŸ [í•µì‹¬ ìˆ˜ì •] ViT ëª¨ë¸ì„ ìœ„í•œ íŠ¹ë³„ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
    if 'vit' in target_name:
        print(f"INFO: ViT model detected. Using direct feature extraction (no hooks).")
        all_features = []
        with torch.no_grad():
            for i in tqdm(range(0, len(data), batch_size)):
                image_input = torch.stack([data[j][0] for j in range(i, min(i + batch_size, len(data)))]).to(device)
                features = target_model(image_input)
                all_features.append(features.cpu())
        
        # ViTëŠ” feature_layerê°€ í•˜ë‚˜ì´ë¯€ë¡œ target_layers[0]ì„ ì‚¬ìš©
        save_name = save_names[target_layers[0]]
        print(f"Saving ViT features to {save_name}")
        torch.save(torch.cat(all_features), save_name)
        return # ViTì˜ ê²½ìš° ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ

    # --- ì´í•˜ ì½”ë“œëŠ” ResNet ë“± ë‹¤ë¥¸ ëª¨ë¸ì„ ìœ„í•œ ê¸°ì¡´ ë¡œì§ ---
    all_features = {}
    for layer in target_layers:
        all_features[layer] = []
    
    hooks = {}
    for target_layer in target_layers:
        command = "target_model.{}.register_forward_hook(get_activation(all_features[target_layer]))".format(target_layer)
        hooks[target_layer] = eval(command)

    with torch.no_grad():
        for i in tqdm(range(0, len(data), batch_size)):
            image_input = torch.stack([data[j][0] for j in range(i, min(i + batch_size, len(data)))]).to(device)
            target_model(image_input)
    
    for target_layer in target_layers:
        torch.save(torch.cat(all_features[target_layer]), save_names[target_layer])
        hooks[target_layer].remove()
#############10/13 ì¶”ê°€ë‚´ìš©
def save_clip_image_features(model, dataset, save_name, batch_size=1000 , device = "cuda"):
    _make_save_dir(save_name)
    all_features = []
    
    if os.path.exists(save_name):
        return
    
    save_dir = save_name[:save_name.rfind("/")]
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    with torch.no_grad():
        for images, labels in tqdm(DataLoader(dataset, batch_size, num_workers=8, pin_memory=True)):
            features = model.encode_image(images.to(device))
            all_features.append(features.cpu())
    torch.save(torch.cat(all_features), save_name)
    #free memory
    del all_features
    torch.cuda.empty_cache()
    return

def save_clip_text_features(model, text, save_name, batch_size=1000):
    
    if os.path.exists(save_name):
        return
    _make_save_dir(save_name)
    text_features = []
    with torch.no_grad():
        for i in tqdm(range(math.ceil(len(text)/batch_size))):
            text_features.append(model.encode_text(text[batch_size*i:batch_size*(i+1)]))
    text_features = torch.cat(text_features, dim=0)
    torch.save(text_features, save_name)
    del text_features
    torch.cuda.empty_cache()
    return

###########10/14 ì¶”ê°€ ë‚´ìš©. 10/14ëŠ” lf-cbmì— remoteclipì ìš©ì„ ìœ„í•´

def save_activations(clip_name, target_name, target_layers, d_probe, 
                     concept_set, batch_size, device, pool_mode, save_dir, remote_clip_path=None):
    # 10/14 remote_clip_path ì¸ì ì¶”ê°€
    
    target_save_name, clip_save_name, text_save_name = get_save_names(clip_name, target_name, 
                                                                    "{}", d_probe, concept_set, 
                                                                      pool_mode, save_dir)
    save_names = {"clip": clip_save_name, "text": text_save_name}
    for target_layer in target_layers:
        save_names[target_layer] = target_save_name.format(target_layer)
        
    if _all_saved(save_names):
        return
    #11/11 í† í¬ë‚˜ì´ì € ì¶”ê°€
    tokenizer = None
    ###########10/14 ì¶”ê°€ ë‚´ìš©. 10/14ëŠ” lf-cbmì— remoteclipì ìš©ì„ ìœ„í•´
    # ğŸŒŸ [í•µì‹¬ ìˆ˜ì •] RemoteCLIP ê²½ë¡œ ìœ ë¬´ì— ë”°ë¼ ëª¨ë¸ ë¡œë”© ë°©ì‹ì„ ë¶„ê¸°í•©ë‹ˆë‹¤.
    if remote_clip_path and os.path.exists(remote_clip_path):
        print("--- Using RemoteCLIP model ---")
        # 1ë‹¨ê³„ì—ì„œ ì¶”ê°€í•œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. clip_nameì€ 'ViT-B-32'ì™€ ê°™ì€ êµ¬ì¡° ì´ë¦„ì…ë‹ˆë‹¤.
        clip_model, clip_preprocess = load_remote_clip(clip_name, remote_clip_path, device)
        # ğŸŒŸ [ìˆ˜ì •] open_clipì˜ í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜´
        tokenizer = open_clip.get_tokenizer(clip_name)
    else:
        print("--- Using standard OpenAI CLIP model ---")
        clip_model, clip_preprocess = clip.load(clip_name, device=device)
        # ğŸŒŸ [ìˆ˜ì •] í‘œì¤€ clipì˜ í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜´
        tokenizer = clip.tokenize
    ###########10/14 ì¶”ê°€ ë‚´ìš©. 10/14ëŠ” lf-cbmì— remoteclipì ìš©ì„ ìœ„í•´

    ##### 10/14 ì‚­ì œ ë‚´ìš© clip_model, clip_preprocess = clip.load(clip_name, device=device)
    
    if target_name.startswith("clip_"):
        target_model, target_preprocess = clip.load(target_name[5:], device=device)
    #11/11 ìˆ˜ì •
    # 1ë‹¨ê³„ì—ì„œ ë³´ë‚¸ "remote_clip_vit_b_32" ì‹ í˜¸ë¥¼ ê°ì§€
    elif target_name == 'remote_clip_vit_b_32':
        if not remote_clip_path: # --remote_clip_pathê°€ ì—†ìœ¼ë©´ ì—ëŸ¬
            raise ValueError("Backbone 'remote_clip_vit_b_32' requires --remote_clip_path")
        
        print("--- Using RemoteCLIP Model as Target Model (for feature extraction) ---")
        # ğŸŒŸ clip_model (RemoteCLIP ì „ì²´)ì„ target_modelë¡œ ì¬ì‚¬ìš©
        target_model = clip_model 
        target_preprocess = clip_preprocess
    #11/11 ìˆ˜ì •ë
    else:
        target_model, target_preprocess = data_utils.get_target_model(target_name, device)
    #setup data
    data_c = data_utils.get_data(d_probe, clip_preprocess)
    data_t = data_utils.get_data(d_probe, target_preprocess)

    with open(concept_set, 'r') as f: 
        words = (f.read()).split('\n')
    # ğŸŒŸ [ìˆ˜ì •] í•˜ë“œì½”ë”©ëœ clip.tokenize ëŒ€ì‹ , ëª¨ë¸ì— ë§ëŠ” 'tokenizer' ë³€ìˆ˜ë¥¼ ì‚¬ìš©
    text = tokenizer(["{}".format(word) for word in words]).to(device)
    
    save_clip_text_features(clip_model, text, text_save_name, batch_size)
    
    save_clip_image_features(clip_model, data_c, clip_save_name, batch_size, device)
    # 11/11 ìˆ˜ì •ì‹œì‘
    if target_name.startswith("clip_") or target_name == 'remote_clip_vit_b_32':
        # ğŸŒŸ [ìˆ˜ì •] remote_clip_vit_b_32 ì‹ í˜¸ë„ ì—¬ê¸°ì„œ ì²˜ë¦¬
        print(f"--- Saving CLIP/RemoteCLIP Image Features for {target_name} ---")
        # ğŸŒŸ RemoteCLIP ëª¨ë¸ì€ encode_imageë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œ
        # (ì£¼ì˜: target_layers[0]ì„ ì‚¬ìš©)
        save_clip_image_features(target_model, data_t, save_names[target_layers[0]], batch_size, device)
    # 11/11 ìˆ˜ì •ë
    else:
        # 10/13 ìˆ˜ì •ë‚´ìš©
        '''
        save_target_activations(target_model, data_t, target_save_name, target_layers,
                                batch_size, device, pool_mode)
        '''
        save_target_activations(target_model, data_t, save_names, target_layers, device,
                            target_name, batch_size)
        # 10/13 ìˆ˜ì •ë‚´ìš©
    
    return
    
    
def get_similarity_from_activations(target_save_name, clip_save_name, text_save_name, similarity_fn, 
                                   return_target_feats=True):
    image_features = torch.load(clip_save_name)
    text_features = torch.load(text_save_name)
    with torch.no_grad():
        image_features /= image_features.norm(dim=-1, keepdim=True).float()
        text_features /= text_features.norm(dim=-1, keepdim=True).float()
        clip_feats = (image_features @ text_features.T)
    del image_features, text_features
    torch.cuda.empty_cache()
    
    target_feats = torch.load(target_save_name)
    similarity = similarity_fn(clip_feats, target_feats)
    
    del clip_feats
    torch.cuda.empty_cache()
    
    if return_target_feats:
        return similarity, target_feats
    else:
        del target_feats
        torch.cuda.empty_cache()
        return similarity
    
def get_activation(outputs, mode):
    '''
    mode: how to pool activations: one of avg, max
    for fc neurons does no pooling
    '''
    if mode=='avg':
        def hook(model, input, output):
            if len(output.shape)==4:
                outputs.append(output.mean(dim=[2,3]).detach().cpu())
            elif len(output.shape)==2:
                outputs.append(output.detach().cpu())
    elif mode=='max':
        def hook(model, input, output):
            if len(output.shape)==4:
                outputs.append(output.amax(dim=[2,3]).detach().cpu())
            elif len(output.shape)==2:
                outputs.append(output.detach().cpu())
    return hook

    
def get_save_names(clip_name, target_name, target_layer, d_probe, concept_set, pool_mode, save_dir):
    
    if target_name.startswith("clip_"):
        target_save_name = "{}/{}_{}.pt".format(save_dir, d_probe, target_name.replace('/', ''))
    else:
        target_save_name = "{}/{}_{}_{}{}.pt".format(save_dir, d_probe, target_name, target_layer,
                                                 PM_SUFFIX[pool_mode])
    clip_save_name = "{}/{}_clip_{}.pt".format(save_dir, d_probe, clip_name.replace('/', ''))
    concept_set_name = (concept_set.split("/")[-1]).split(".")[0]
    text_save_name = "{}/{}_{}.pt".format(save_dir, concept_set_name, clip_name.replace('/', ''))
    
    return target_save_name, clip_save_name, text_save_name

    
def _all_saved(save_names):
    """
    save_names: {layer_name:save_path} dict
    Returns True if there is a file corresponding to each one of the values in save_names,
    else Returns False
    """
    for save_name in save_names.values():
        if not os.path.exists(save_name):
            return False
    return True

def _make_save_dir(save_name):
    """
    creates save directory if one does not exist
    save_name: full save path
    """
    save_dir = save_name[:save_name.rfind("/")]
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    return
########### 10/3 ì¶”ê°€ ë‚´ìš©
def get_accuracy_cbm(model, dataset, device, batch_size=250, num_workers=0): # num_workers=0ìœ¼ë¡œ ê³ ì •
    correct = 0
    total = 0
    
    loader = DataLoader(dataset, batch_size, num_workers=num_workers, pin_memory=True)

    with torch.no_grad():
        for images, labels in tqdm(loader):
            images = images.to(device)

            outs, _ = model(images)
            
            # ğŸŒŸ [í•µì‹¬ ìˆ˜ì •] ì¶œë ¥ í…ì„œì˜ ëª¨ì–‘ì„ í™•ì¸í•˜ê³ , í•„ìš”í•˜ë©´ ì „ì¹˜(transpose)í•©ë‹ˆë‹¤.
            if outs.shape[0] != images.shape[0]:
                outs = outs.T # .TëŠ” .transpose(0, 1)ê³¼ ê°™ìŠµë‹ˆë‹¤.

            pred = torch.argmax(outs, dim=1).cpu()

            correct += torch.sum(pred == labels)
            total += len(labels)
            
    return (correct.item() / total) if total > 0 else 0.0
########### 10/3 ì¶”ê°€ ë‚´ìš©
def get_preds_cbm(model, dataset, device, batch_size=250, num_workers=2):
    preds = []
    for images, labels in tqdm(DataLoader(dataset, batch_size, num_workers=num_workers,
                                           pin_memory=True)):
        with torch.no_grad():
            outs, _ = model(images.to(device))
            pred = torch.argmax(outs, dim=1)
            preds.append(pred.cpu())
    preds = torch.cat(preds, dim=0)
    return preds

def get_concept_act_by_pred(model, dataset, device):
    preds = []
    concept_acts = []
    for images, labels in tqdm(DataLoader(dataset, 500, num_workers=8, pin_memory=True)):
        with torch.no_grad():
            outs, concept_act = model(images.to(device))
            concept_acts.append(concept_act.cpu())
            pred = torch.argmax(outs, dim=1)
            preds.append(pred.cpu())
    preds = torch.cat(preds, dim=0)
    concept_acts = torch.cat(concept_acts, dim=0)
    concept_acts_by_pred=[]
    for i in range(torch.max(pred)+1):
        concept_acts_by_pred.append(torch.mean(concept_acts[preds==i], dim=0))
    concept_acts_by_pred = torch.stack(concept_acts_by_pred, dim=0)
    return concept_acts_by_pred
